{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn as skl\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, RepeatedKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error as mse, r2_score as r2\nfrom sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Lasso, LassoCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom datetime import datetime\nimport xgboost as xgb\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Загружаем данные ","metadata":{}},{"cell_type":"code","source":"TRAIN_DATASET_PATH = '/kaggle/input/real-estate-price-prediction-moscow/train.csv'\nTEST_DATASET_PATH = '/kaggle/input/real-estate-price-prediction-moscow/test.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATASET_PATH)\ntest_df = pd.read_csv(TEST_DATASET_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape[1] - 1 == test_df.shape[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Распределение целевой переменной и корреляции","metadata":{}},{"cell_type":"code","source":"def correlation_matrix(df):\n    plt.figure(figsize=(20,20))\n    sns.set(font_scale=1.4)\n    corr_matrix = df.corr()\n    corr_matrix = np.round(corr_matrix, 2)\n    corr_matrix[np.abs(corr_matrix)<0.3] = 0\n    sns.heatmap(corr_matrix, annot=True, linewidth=.5, cmap=\"coolwarm\")\n    plt.title(\"Матрица корреляции\")\n    plt.plot()\ncorrelation_matrix(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# посмотрим подробнее на целевую переменную\n\ndef deviations(series):\n    print(series.describe())\n    median = series.quantile(q=0.5)\n    avg = series.mean()\n    std = series.std()\n    mode = series.mode()[0]\n    print(f\"Медиана со средним значением отличаются на {round(abs(median-avg)/std, 2)} стандартных отклонения\")\n    if series.min() < median-std*3:\n        print(f\"Минимальное значение меньше, чем медиана - 3 стандартных отклонения (выходит из 99,7% диапазона данных)\")\n    if series.max() > median+std*3:\n        print(f\"Максимальное значение больше, чем медиана + 3 стандартных отклонения (выходит из 99,7% диапазона данных)\")\n    return median, avg, mode\n\nmedian, avg, mode = deviations(train_df[\"Price\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (16, 8))\n\ntrain_df['Price'].hist(bins=100)\nplt.ylabel('Count')\nplt.xlabel('Price')\nplt.axvline(median, c=\"red\", label=\"median\")\nplt.axvline(avg, c=\"green\", label=\"average\")\nplt.legend(loc=\"best\", frameon=False)\n\nplt.title('Target distribution')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation = train_df.corrwith(train_df['Price']).sort_values(ascending=False)\ncorrelation.drop('Price', inplace=True)\n\nplt.figure(figsize = (16, 8))\nplt.bar(correlation.index, correlation)\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Correlation', fontsize=15)\nplt.title('Feature correlation', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Обработка данных и генерация признаков","metadata":{}},{"cell_type":"code","source":"class DataPreprocessor:\n    def __init__(self):\n        self.medians = None\n        self.kitchen_square_max_value = None\n        self.kitchen_square_min_value = None\n        self.life_square_min_value = None\n        self.max_floor = None\n        self.districts_healthcare = None\n\n\n        \n    def fit(self, df):\n        # Medians and quantiles\n        self.medians = df.median() # medians\n        self.kitchen_square_max_value = np.quantile(df['KitchenSquare'], q=0.999) \n        self.kitchen_square_min_value = np.quantile(df['KitchenSquare'], q=0.001) \n        self.life_square_min_value = np.quantile(df['LifeSquare'], q=0.03) \n        self.max_floor = df['Floor'].max()\n        \n        \n        # Compute mean Helthcare_1 value in each district\n        self.districts_healthcare = df.groupby(['DistrictId'])['Healthcare_1'].agg('mean').to_dict()\n\n\n    def transform(self, df):\n        # Life Square fillna\n        df['LifeSquare'].fillna(((df['Square'] - df['KitchenSquare']) - df['Square']*0.2), inplace=True)\n\n\n        # Rooms\n        # Fillna with medians         \n        df['Rooms'].fillna(self.medians.Rooms, inplace=True) \n        \n        # Compute median room square and fill outliers with LifeSquare/Room_square \n        condition_rooms = (df['Rooms'] > 6) | (df['Rooms'] == 0)   \n        room_sq = np.round((self.medians.LifeSquare / self.medians.Rooms, 1))[0]\n        df.loc[condition_rooms , 'Rooms'] = df.loc[condition_rooms, 'LifeSquare'] / room_sq\n\n        # Square\n        # If LifeSquare > Square: exchange values\n        df['Square'], df['LifeSquare'] = np.where(df['Square'] < df['LifeSquare'],(df['LifeSquare'],df['Square']), (df['Square'],df['LifeSquare']))\n\n\n        # LifeSquare\n        # Fill outliers with (Square - KithcenSquare)\n        ls_condition = (df['LifeSquare'] < self.life_square_min_value)\n        df.loc[ls_condition, 'LifeSquare'] = df.loc[ls_condition, ['Square']] - df.loc[ls_condition, ['KitchenSquare']]\n  \n        \n        # KitchenSquare\n        # Fill outliers with (Square - LifeSquare - 10% of Square)\n        condition_kitchen_square = (df['KitchenSquare'] > self.kitchen_square_max_value) | (df['KitchenSquare'] < self.kitchen_square_min_value)\n        df.loc[condition_kitchen_square, 'KitchenSquare'] = df.loc[condition_kitchen_square, 'Square'] - df.loc[condition_kitchen_square, 'LifeSquare'] \\\n        - (df.loc[condition_kitchen_square, 'Square'] * 0.1)\n\n\n        # Ecology and Shops\n        # Switch to binary\n        df.replace({'Ecology_2': {'A': 0, 'B': 1}}, inplace=True)\n        df.replace({'Ecology_3': {'A': 0, 'B': 1}}, inplace=True)\n        df.replace({'Shops_2': {'A': 0, 'B': 1}}, inplace=True)\n        \n        \n        # HouseFloor\n        # If HouseFloor < Floor: exchange values\n        house_floor_condition = df['HouseFloor'] < df['Floor']\n        df.loc[house_floor_condition, 'HouseFloor'] = df.loc[house_floor_condition, 'Floor']\n\n        \n        # HouseYear\n        # If HouseYear > current year set median value\n        current_year = datetime.now().year\n        condition_year = (df['HouseYear'] > current_year)\n        df.loc[condition_year, 'HouseYear'] = self.medians.HouseYear\n        \n        \n        # Healthcare\n        # Fillna with dictrict healthcare value. (If district has no healthcare value fill with medians)      \n        df.loc[df['Healthcare_1'].isna(), 'Healthcare_1'] = df['DistrictId'].map(self.districts_healthcare)\n        df['Healthcare_1'].fillna(self.medians.Healthcare_1, inplace=True)       \n        # Clip on upper quantille\n        q_max = np.quantile(df['Healthcare_1'], q=0.9)\n        df['Healthcare_1'].clip(upper=q_max, axis=0, inplace=True)\n        \n        \n        # Drop Id\n        df.drop(['Id'], axis=1, inplace=True)\n\n        \n        # Fillna just in case\n        df.fillna(self.medians, inplace=True)\n        return df\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeatureGenerator:\n    def __init__(self):\n        self.districts_median_year = None\n        self.districts_density = None\n        self.district_price_per_square = None\n        self.min_sq = None\n        self.max_sq = None\n\n        \n    def fit(self, df):   \n        self.min_sq = np.quantile(df['Square'], q=0.005)\n        self.max_sq = np.quantile(df['Square'], q=0.995)\n        \n        self.districts_median_year = df.groupby(['DistrictId'])['HouseYear'].agg('median').to_dict()  # median house year in each district\n        self.districts_density = df.groupby(['DistrictId'])['Square'].agg('median').to_dict()  # median square in each district\n        self.district_price_per_square = df.groupby(['DistrictId'])['Price'].agg('median') \\\n        / df.groupby(['DistrictId'])['Square'].agg('median')  # median price for square meter in each district\n        \n    \n    def new_features(self, df):\n        # How old is the district      \n        df['DistrictYear'] = df['DistrictId'].map(self.districts_median_year)\n        \n        self.median_district_year = df['DistrictYear'].median()\n        df['DistrictYear'].fillna(self.median_district_year, inplace=True)\n\n\n        # Median square of flat in each district\n        df['DistrictDensity'] = df['DistrictId'].map(self.districts_density)\n        \n        self.median_district_density = df['DistrictDensity'].median()\n        df['DistrictDensity'].fillna(self.median_district_density, inplace=True)\n\n\n        # Median price for square meter in each district\n        self.district_price_per_square.to_dict()\n        df['DistrictPrice'] = df['DistrictId'].map(self.district_price_per_square)\n        \n        self.median_district_price_per_square = df['DistrictPrice'].median()\n        df['DistrictPrice'].fillna(self.median_district_price_per_square, inplace=True)\n\n\n        # Floor category\n        floor_bins = [0, 4, 7, 12, df['Floor'].max()]\n        df['Floor_cat'] = pd.cut(df['Floor'], bins=floor_bins, labels=False)\n        df['Floor_cat'].fillna(-1, inplace=True) \n\n        \n        # Scale and merge Social\n        scaler = RobustScaler()\n        pca = PCA(n_components=1, random_state=42)\n        social_scaled = pd.DataFrame(scaler.fit_transform(df[['Social_1', 'Social_2', 'Social_3']]))\n        df['Social'] = pca.fit_transform(social_scaled)\n        \n        \n        df.drop(['Ecology_2', 'Ecology_3', 'Shops_2', 'Helthcare_2', 'Floor',], axis=1, inplace=True)\n        \n        return df\n    \n    \n    def drop_outliers(self, df):\n        df = df.loc[(df['Square'] > self.min_sq) & (df['Square'] < self.max_sq)]\n        \n        \n        \n        return df\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предобработка данных","metadata":{}},{"cell_type":"code","source":"preprocessor = DataPreprocessor()\npreprocessor.fit(train_df)\ntrain_df = preprocessor.transform(train_df)\ntest_df = preprocessor.transform(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Генерация признаков","metadata":{}},{"cell_type":"code","source":"features_gen = FeatureGenerator()\nfeatures_gen.fit(train_df)\ntrain_df = features_gen.new_features(train_df)\ntrain_df = features_gen.drop_outliers(train_df)\ntest_df = features_gen.new_features(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Разбиение признаков","metadata":{}},{"cell_type":"code","source":"y = pd.DataFrame(data=train_df['Price'])\ntrain_df.drop('Price', axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"gb_model = GradientBoostingRegressor(criterion='mse',\n                                     max_depth=6,\n                                     min_samples_leaf=50,\n                                     random_state=42,  \n                                     n_estimators=2250, \n                                     max_features='sqrt', \n                                     loss='huber', \n                                     learning_rate=0.025)\n\ngb_model.fit(X_train, y_train)\n\ny_train_preds = gb_model.predict(X_train)\ny_test_preds = gb_model.predict(X_test)\nprint(r2(y_train, y_train_preds))\nprint(r2(y_test, y_test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Важность признаков","metadata":{}},{"cell_type":"code","source":"feature_importances = pd.DataFrame(zip(X_train.columns, \n                                       gb_model.feature_importances_), \n                                   columns=['feature_name', 'importance'])\n\nfeature_importances.sort_values(by='importance', ascending=False, inplace=True)\nfeature_importances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (16, 8))\nplt.bar(feature_importances['feature_name'], feature_importances['importance'])\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Importance', fontsize=15)\nplt.title('Feature importances', fontsize=15)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\n\nmodel = CatBoostRegressor(task_type=\"CPU\", early_stopping_rounds=10, od_type=\"IncToDec\", eval_metric=\"R2\", loss_function=\"RMSE\")\ngrid = {'learning_rate': np.arange(0.5,1,.05),\n        'depth': np.arange(5,9,1),\n        'l2_leaf_reg': np.arange(1000,3000,1000)}\n\ngrid_search_result = model.grid_search(grid, \n                                       X=X_train, \n                                       y=y_train, \n                                       cv=5,\n                                       calc_cv_statistics=True,\n                                       search_by_train_test_split=True,\n                                       refit=True,\n                                       shuffle=True,\n                                       train_size=0.8,\n                                       verbose=True,\n                                       plot=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_results = grid_search_result[\"params\"]\nbest_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred_train = model.predict(X_train)\nfrom sklearn.metrics import r2_score\nprint(\"Train: \", r2_score(y_train, y_pred_train))\nprint(\"Test:\", r2_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Предсказание на неизвестных данных и подготовка к выгрузке на кагл","metadata":{}},{"cell_type":"code","source":"test_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv('/kaggle/input/real-estate-price-prediction-moscow/sample_submission.csv')\nsubmit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_df)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit['Price'] = predictions\nsubmit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv('gb_submit.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}