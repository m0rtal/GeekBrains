Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
Micro используется когда важно качество (accuracy) и нет дисбаланса классов, Macro когда каждый класс важен, Weighted когда важно предсказать более "тяжлый" класс.

В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
LightGBM использует технику односторонней выборки на основе градиента (Gradient-Based One Side Sampling) для нахождения значения разделения, в то время как XGBoost использует предварительно отсортированный алгоритм и алгоритм на основе гистограммы для вычисления наилучшего разделения. CatBoost обладает уникальным методом переопределения категориальных данных, что повышает его эффективность при работе с такими данными.

XGBoost:
depth-wise boosting

LightGBM:
Leaf-wise boosting
Очень быстрый
Понимает категориальные признаки

CatBoost:
Leaf-wise boosting
Понимает категориальные признаки
Понимает текстовые признаки (через механизм хэша)
Богат параметрами, но тяжёл в настройке
